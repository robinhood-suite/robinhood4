{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "5c2b6a93_a986cc6a",
        "filename": "rbh-fsevents/src/deduplicator.c",
        "patchSetId": 2
      },
      "lineNbr": 46,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "defect: I do not understand what happens to the fsevent that generates the ENOSPC. On one side, the deduplicator-\u003esource-\u003efsevents iterator has already returns it and, I guess, will never return it again. On the other side, the implementation of the rbh_fsevent_pool_push function simply returns ENOSPC without storing it. So, I have the impression that this fsevent is simply lost and never register. Where am I wrong ?\n\nI guess a correct implementation of the rbh_fsevent_pool_push is to return ENOSPC as result of the call adding the last entry instead of returning ENOSPC in the call that try to add the entry next to the last stored one.\n\n(This behaviour bans pool of size 0.)",
      "range": {
        "startLine": 45,
        "startChar": 9,
        "endLine": 46,
        "endChar": 24
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a86ba173_d9899c2e",
        "filename": "rbh-fsevents/src/deduplicator.c",
        "patchSetId": 2
      },
      "lineNbr": 46,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T11:23:11Z",
      "side": 1,
      "message": "I think you are right, we are loosing one event each time the pool is full.",
      "parentUuid": "5c2b6a93_a986cc6a",
      "range": {
        "startLine": 45,
        "startChar": 9,
        "endLine": 46,
        "endChar": 24
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "3a2e079c_1ee0cc4d",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 22,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "major: In the function rbh_fsevent_pool_is_full, you use this value as the threshold to the fact that the pool is full, not the number of element to flushed ?\n\nWhy did you not store pool_size (first arg of the pool_new function) as the total size of the pool, use it in is_full, and store flush_size as the number of entries to flush as you describe in this comment ?\n\nstatic bool\nrbh_fsevent_pool_is_full(struct rbh_fsevent_pool *pool)\n{\n    return pool-\u003ecount \u003d\u003d pool-\u003eflush_size;\n}",
      "range": {
        "startLine": 21,
        "startChar": 4,
        "endLine": 22,
        "endChar": 26
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ef2e6580_2ac2f14c",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 22,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T11:23:11Z",
      "side": 1,
      "message": "Indeed, I should check that the total size of the pool is reached not the flush size.",
      "parentUuid": "3a2e079c_1ee0cc4d",
      "range": {
        "startLine": 21,
        "startChar": 4,
        "endLine": 22,
        "endChar": 26
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "66318d5b_6cfc753d",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 58,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "minor: should not we add a check to prohibit creation of pool with pool_size of 0 ?",
      "range": {
        "startLine": 58,
        "startChar": 0,
        "endLine": 58,
        "endChar": 1
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a032e9a7_d13c93cc",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 155,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "comment: (linked to the defect at ENOSPC) isn\u0027t it the good place to check an return ENOSPC ?",
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d11d850b_ce7d8daa",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 155,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T11:23:11Z",
      "side": 1,
      "message": "Yes, this would prevent the loss of an event.",
      "parentUuid": "a032e9a7_d13c93cc",
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "95c432c5_05e00b42",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 155,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T13:29:28Z",
      "side": 1,
      "message": "One thing that I am noting here is that this does not prevent the caller from pushing too many events. Meaning that if I check if the pull is full after the push is done, a caller that calls push again when errno was set to ENOSPC will be able to add more elements. Maybe the implementation of push should be left as is to prevent overflowing the pool and then the caller has to push the fsevent again after the flush. We could do something like this:\n\n```\nlast_fsevent \u003d NULL;\n\nloop {\n    fsevent \u003d iter_next();\n    \n    push(fsevent)\n    if (errno \u003d\u003d ENOSPC) {\n        last_fsevent \u003d fsevent;\n        break;\n    }\n    ...\n}\n\niterator \u003d flush(pool);\nif last_fsevent\n    push(last_fsevent);\n\nreturn iterator;\n```\n\nAn other solution is to keep both checks (before the actual push and after) so that after a push, we return ENOSPC to the caller to indicate that the next push won\u0027t succeed. So maybe we can have:\n- on the last push that filled the pool (i.e. full after insertion):\n  return 0 and set errno to ENOSPC (which is a bit weird to do since the function returns 0)\n- on the next push (i.e. the pool is already full before insert):\n  do not insert the fsevent, return -1 and set errno to ENOSPC\n  \nThe second solution is easier to use from the caller\u0027s perspective and does not require too many changes to the current implementation.\n\nWhat to you think?",
      "parentUuid": "d11d850b_ce7d8daa",
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "becb6173_fa9aae8c",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 191,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "comment: (linked to the defect ENOSPC) ENOSPC is returned without storing event anywhere in the pool.",
      "range": {
        "startLine": 188,
        "startChar": 4,
        "endLine": 191,
        "endChar": 5
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e3c46141_baa8fc1f",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 219,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T10:36:40Z",
      "side": 1,
      "message": "major: why cleaning the flushed list of event when you try to create a new one instead of cleaning each element when you return it by the iterator ?",
      "range": {
        "startLine": 218,
        "startChar": 4,
        "endLine": 219,
        "endChar": 34
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9cba5416_197d1187",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 219,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T11:23:11Z",
      "side": 1,
      "message": "Do you mean that the list iterator returned at the end of this function would remove elements from the list as they are yield to the caller ?",
      "parentUuid": "e3c46141_baa8fc1f",
      "range": {
        "startLine": 218,
        "startChar": 4,
        "endLine": 219,
        "endChar": 34
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e753c600_ee2572f6",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 219,
      "author": {
        "id": 1008040
      },
      "writtenOn": "2023-09-07T12:59:19Z",
      "side": 1,
      "message": "Yes. I\u0027m not familiar with the rbh iterators. But isn\u0027t it the rigth way to proceed ?\n\nAre you sure when pool_flush is called that you can clear the list ? Are you sure that the iterator of the prev flush is finished and already send all events elem ?\n\nI\u0027m just trying to understand how you manage the previous iterator returned by previous flush and the new one and the cleaning ...",
      "parentUuid": "9cba5416_197d1187",
      "range": {
        "startLine": 218,
        "startChar": 4,
        "endLine": 219,
        "endChar": 34
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "44359e1c_a243f5dc",
        "filename": "rbh-fsevents/src/deduplicator/fsevent_pool.c",
        "patchSetId": 2
      },
      "lineNbr": 219,
      "author": {
        "id": 1019142
      },
      "writtenOn": "2023-09-07T13:29:28Z",
      "side": 1,
      "message": "Everything is sequential here. Which means that when flush is first called and returns an iterator, the function feed will call rbh_backend_update (through the function sink_process -\u003e backend_sink_process -\u003e rbh_backend_upadte). This function will iterate over the list_iter returned here (there is also the enrich process that may happen in between but lets ignore it since it does not change the logic I\u0027m describing here) and perform the necessary updates to the backend. Once all the events of the list are processed, the sink_process will return and feed will call rbh_iter_next on the deduplicator which in turn will call rbh_iter_next on the source to get more changelogs from the reader.\n\nOnce the pool is full again, we call flush, this time we have elements in the list that we remove. This is fine since all the updates are already committed to the backend. We could create an iterator that fetches an element from the list then removes it and returns the associated value. I don\u0027t know what\u0027s best between the two. This solution is maybe a bit more explicit but relies on the fact that all the events are committed between each call to flush. Which is the case for now.",
      "parentUuid": "e753c600_ee2572f6",
      "range": {
        "startLine": 218,
        "startChar": 4,
        "endLine": 219,
        "endChar": 34
      },
      "revId": "c690b582be42f25bcf211832643e1b7ffc74aefd",
      "serverId": "d5d70762-12d0-45a1-890d-524b12d3f735"
    }
  ]
}